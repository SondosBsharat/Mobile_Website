<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>MMLU-Mobile Bench</title>
    <link href="https://fonts.googleapis.com/css2?family=Roboto:wght@400;700&display=swap" rel="stylesheet">
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/6.0.0-beta3/css/all.min.css">
    <link rel="stylesheet" href="styles2.css">
</head>
<body>
    <header>
        <h1>Mobi-Genesis</h1>
        <p>A Mobile Intelligence Massive Multitask Language Understanding Benchmark</p>
        <div class="header-buttons">
            <a href="#home"><i class="fa fa-home"></i>Home</a>
            <a href="#leaderboard"><i class="fa fa-chart-bar"></i>Leaderboard</a>
            <a href="#github"><i class="fa fa-github"></i>GitHub</a>
            <a href="#contact"><i class="fa fa-envelope"></i>Contact</a>
            <a href="#x"><i class="fa fa-times"></i>X</a>
        </div>
    </header>

    <main class="content">
        <section id="intro">
            <h2>Welcome to Mobi-Genesis</h2>
            <p>Your gateway to evaluating mobile-compatible Large Language Models (LLMs) across 80 diverse fields such as Education, Healthcare, and Technology. Mobi-Genesis is redefining mobile intelligence benchmarks for a smarter future.</p>
            <a href="#benchmark" class="cta-button">Explore Benchmarks</a>
        </section>

        

        <div class="feature-section">
            <div class="feature">
                <h3>Motivation</h3>
                <p>Our Motivation (Draft)</p>
                 <div class="methodology-feature">
                    <p><strong>Why Apple Intelligence and Mobile Intelligence Are Important for LLMs on Mobile Devices:</strong></p>
                    <ol class="methodology-list">
      <p><strong><!-- Why Apple Intelligence and Mobile Intelligence Are Important for LLMs on Mobile Devices: --></strong></p>
	<li><strong>1. On-Device Efficiency:</strong>
Mobile devices, such as smartphones, tablets, and wearables, have limited computational resources compared to high-end servers. Apple Intelligence and other mobile-focused AI frameworks prioritize optimization techniques to run small-scale large language models (LLMs) directly and efficiently on these constrained environments. Techniques like model compression, quantization, and efficient architectures (e.g., linear complexity models) are crucial for enabling LLMs to perform effectively on mobile devices. However, these models have limited capabilities.
	<li><strong>2.	Personalization and Privacy:</strong>
On-device LLMs can process user data locally without sending sensitive information to cloud servers. This ensures better privacy and security, a feature that aligns with Appleâ€™s emphasis on user-centric privacy through its Apple Intelligence ecosystem. Mobile intelligence allows models to adapt to user preferences, habits, and behaviors in real-time while safeguarding data.
	<li><strong>3.	Low Latency:</strong>
Running LLMs directly on mobile devices eliminates the need for constant internet connectivity and reduces reliance on cloud infrastructure. This enables real-time responses and faster performance, which are critical for applications such as voice assistants, augmented reality, and on-device content generation.
	<li><strong>4.	Accessibility and Ubiquity:</strong>
Mobile devices are used globally and form a significant part of everyday life. Integrating LLMs into mobile platforms broadens their accessibility, enabling users in low-bandwidth regions or offline environments to experience the benefits of AI seamlessly. Apple Intelligence exemplifies how optimized LLMs can democratize advanced capabilities for mainstream users.
</li> 

 <div class="methodology-feature">
    <p><strong>Why a Mobile LLM Benchmark Is Necessary to Identify Good Mobile LLMs:</strong></p>
    <ol class="methodology-list">
	<li><strong>1.	Performance Evaluation in Realistic Settings:</strong>
A dedicated mobile LLM benchmark is essential to evaluate models under the real-world limitations of mobile devices, such as limited memory, energy constraints, and computational power. Traditional benchmarks often target cloud-based models without accounting for these restrictions.
	<li><strong>2.	Model Comparability:</strong>
A standard benchmark allows researchers and developers to compare different LLM architectures, optimizations, and compression techniques fairly. Without such benchmarks, it is challenging to assess which models are suitable for mobile deployment.
	<li><strong>3.	Identifying Use-Specific Solutions:</strong>
Mobile devices are integral to daily life, and integrating LLMs allows for personalized, real-time experiences such as smarter virtual assistants, contextual recommendations, and language understanding for apps. Apple Intelligence, for example, enables LLMs to adapt to user behaviors, provide tailored responses, and improve applications like Siri, search, and messaging.
	<li><strong>4.	Driving Innovation:</strong>
A benchmark provides clear goals for researchers and companies to innovate further. By setting performance, efficiency, and latency baselines, it encourages the development of new algorithms, architectures, and optimization strategies tailored to mobile platforms.
	<li><strong>5.	Accuracy vs. Efficiency Trade-off:</strong>
Mobile LLMs require a balance between accuracy and resource efficiency. Benchmarks can identify the best-performing models that achieve high accuracy while maintaining fast inference times and low energy consumption on mobile hardware.
	<li><strong>6.	Improving End-User Experience:</strong>
Ultimately, a mobile LLM benchmark helps identify models that deliver the best user experience on mobile devices. This includes fluid interactions, responsiveness, and minimal battery drain, making LLM-powered features practical and usable on a large scale.

   
<!--
                On-Device Processing: Apple Intelligence and similar mobile intelligence frameworks focus on optimizing computational efficiency to enable advanced AI tasks directly on mobile devices. This is crucial for LLMs, which are typically resource-intensive and require significant memory, computation, and energy. Efficient on-device LLMs can perform tasks without relying on constant cloud connectivity, improving responsiveness and privacy.
                
                Enhanced User Experience: Mobile devices are integral to daily life, and integrating LLMs allows for personalized, real-time experiences such as smarter virtual assistants, contextual recommendations, and language understanding for apps. Apple Intelligence, for example, enables LLMs to adapt to user behaviors, provide tailored responses, and improve applications like Siri, search, and messaging.
-->
                <img src="Images/benchmark_results.png" alt="Motiviation">
            </div>
            <div class="feature">
                <h3>Benchmark Results</h3>
                <p>Discover the latest results from our interactive visualizations, comparing LLMs on performance, accuracy, and efficiency. Dive deep into the metrics and make informed decisions about the future of mobile intelligence.</p>
                <img src="Images/mm.png" alt="Benchmark Results Visualization">
            </div>

            <div class="feature">
                <h3>Our Methodology</h3>
                <p>Our methodology reflects a rigorous and systematic approach, designed to ensure not only quality but also practicality for mobile-based applications. By leveraging detailed planning and execution strategies, we aim to provide benchmarks that resonate with real-world use cases. Every step has been  designed and reviewed to uphold the highest standards of relevance, reliability, and scalability.</p>
                <div class="methodology-feature">
                    <p><strong>Our methodology is a detailed, multi-step process designed to ensure comprehensive and reliable benchmarks:</strong></p>
                    <ol class="methodology-list">
                        <li><strong>Field Selection:</strong> We began by conducting an in-depth search to identify fields that people frequently need or use in daily life, work, shopping, gaming, travel, or other scenarios. These fields were designed to align with mobile searches and user queries. These fields gathered from diverse sources including Wikipedia, various websites, and large language models to ensure inclusivity and relevance.</li>
                        <li><strong>Question Structuring and Human Annotation:</strong> 
                            <ul>
                                <li>Questions were categorized into two types: standard questions and challenging scenario-based questions.</li>
                                <li>The questions underwent multiple rounds of human annotation. This included generating the ground truth answers first and then creating multiple-choice questions (MCQs) based on the ground truth. The MCQs were crafted with the following principles:
                                    <ul>
                                        <li>Options were highly similar to the ground truth, differing only in specific keywords or subtle details to make them incorrect.</li>
                                        <li>On average, MCQs were longer than the ground truth answers to test model precision.</li>
                                        <li>Some questions included multiple correct answers for added complexity.</li>
                                    </ul>
                                </li>
                            </ul>
                        </li>
                        <li><strong>Quality Assurance:</strong> The generated questions were thoroughly reviewed for similarity and uniqueness. Any redundant or overly similar questions were removed. Additionally, each batch of questions underwent sampling and human verification to ensure accuracy and relevance.</li>
                        <li><strong>Evaluation on LLMs:</strong> The curated dataset was used to evaluate various large language models across different scales, focusing particularly on those optimized for mobile usage. Evaluation metrics included latency, accuracy, and energy efficiency to ensure the benchmarks were practical for mobile environments.</li>
                    </ol>
                    <p><em>This meticulous process ensures that our benchmarks are not only comprehensive but also reflective of real-world mobile usage scenarios.</em></p>
                    <img src="Images/pipeline_simple.jpg" alt="Methodology Overview">
                </div>
            </div>
            

            <div class="feature">
                <h3>Data Analysis</h3>
                <p>Explore our comprehensive dataset covering 80 fields, from technical disciplines to creative domains. Gain insight into the diversity and depth of the data that drives our benchmarks.</p>
                <img src="Images/data_fields.png" alt="Data Analysis Overview">
            </div>
        </div>
    </main>

    <footer>
        <p>&copy; 2024 MMLU-Mobile Bench. All rights reserved.</p>
    </footer>
</body>
</html>
