<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>MMLU-Mobile Bench</title>
    <link href="https://fonts.googleapis.com/css2?family=Roboto:wght@400;700&display=swap" rel="stylesheet">
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/6.0.0-beta3/css/all.min.css">
    <link rel="stylesheet" href="styles2.css">
</head>
<body>
    <header>
        <h1>Mobi-Genesis</h1>
        <p>A Mobile Intelligence Massive Multitask Language Understanding Benchmark</p>
        <div class="header-buttons">
            <a href="#home"><i class="fa fa-home"></i>Home</a>
            <a href="#leaderboard"><i class="fa fa-chart-bar"></i>Leaderboard</a>
            <a href="#github"><i class="fa fa-github"></i>GitHub</a>
            <a href="#contact"><i class="fa fa-envelope"></i>Contact</a>
            <a href="#x"><i class="fa fa-times"></i>X</a>
        </div>
    </header>

    <main class="content">
        <section id="intro">
            <h2>Welcome to Mobi-Genesis</h2>
            <p>Your gateway to evaluating mobile-compatible Large Language Models (LLMs) across 80 diverse fields such as Education, Healthcare, and Technology. Mobi-Genesis is redefining mobile intelligence benchmarks for a smarter future.</p>
            <a href="#benchmark" class="cta-button">Explore Benchmarks</a>
        </section>

        

        <div class="feature-section">
            <div class="feature">
                <h3>Motivation</h3>
                <p>Our Motivation</p>
                <img src="Images/benchmark_results.png" alt="Motiviation">
            </div>
            <div class="feature">
                <h3>Benchmark Results</h3>
                <p>Discover the latest results from our interactive visualizations, comparing LLMs on performance, accuracy, and efficiency. Dive deep into the metrics and make informed decisions about the future of mobile intelligence.</p>
                <img src="Images/mm.png" alt="Benchmark Results Visualization">
            </div>

            <div class="feature">
                <h3>Our Methodology</h3>
                <p>Our methodology reflects a rigorous and systematic approach, designed to ensure not only quality but also practicality for mobile-based applications. By leveraging detailed planning and execution strategies, we aim to provide benchmarks that resonate with real-world use cases. Every step has been  designed and reviewed to uphold the highest standards of relevance, reliability, and scalability.</p>
                <div class="methodology-feature">
                    <p><strong>Our methodology is a detailed, multi-step process designed to ensure comprehensive and reliable benchmarks:</strong></p>
                    <ol class="methodology-list">
                        <li><strong>Field Selection:</strong> We began by conducting an in-depth search to identify fields that people frequently need or use in daily life, work, shopping, gaming, travel, or other scenarios. These fields were designed to align with mobile searches and user queries. These fields gathered from diverse sources including Wikipedia, various websites, and large language models to ensure inclusivity and relevance.</li>
                        <li><strong>Question Structuring and Human Annotation:</strong> 
                            <ul>
                                <li>Questions were categorized into two types: standard questions and challenging scenario-based questions.</li>
                                <li>The questions underwent multiple rounds of human annotation. This included generating the ground truth answers first and then creating multiple-choice questions (MCQs) based on the ground truth. The MCQs were crafted with the following principles:
                                    <ul>
                                        <li>Options were highly similar to the ground truth, differing only in specific keywords or subtle details to make them incorrect.</li>
                                        <li>On average, MCQs were longer than the ground truth answers to test model precision.</li>
                                        <li>Some questions included multiple correct answers for added complexity.</li>
                                    </ul>
                                </li>
                            </ul>
                        </li>
                        <li><strong>Quality Assurance:</strong> The generated questions were thoroughly reviewed for similarity and uniqueness. Any redundant or overly similar questions were removed. Additionally, each batch of questions underwent sampling and human verification to ensure accuracy and relevance.</li>
                        <li><strong>Evaluation on LLMs:</strong> The curated dataset was used to evaluate various large language models across different scales, focusing particularly on those optimized for mobile usage. Evaluation metrics included latency, accuracy, and energy efficiency to ensure the benchmarks were practical for mobile environments.</li>
                    </ol>
                    <p><em>This meticulous process ensures that our benchmarks are not only comprehensive but also reflective of real-world mobile usage scenarios.</em></p>
                    <img src="Images/pipeline_simple.jpg" alt="Methodology Overview">
                </div>
            </div>
            

            <div class="feature">
                <h3>Data Analysis</h3>
                <p>Explore our comprehensive dataset covering 80 fields, from technical disciplines to creative domains. Gain insight into the diversity and depth of the data that drives our benchmarks.</p>
                <img src="Images/data_fields.png" alt="Data Analysis Overview">
            </div>
        </div>
    </main>

    <footer>
        <p>&copy; 2024 MMLU-Mobile Bench. All rights reserved.</p>
    </footer>
</body>
</html>
